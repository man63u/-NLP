## RNN
### 特点：记忆功能

![RNN Image](https://github.com/user-attachments/assets/9f67d2d3-d7f9-465a-808a-4f3a0b2da26c)

### RNN结构
- \( X_t \)：时间 \( t \) 处的输入
- \( S_t \)：时间 \( t \) 处的记忆  
  \[
  S_t = f(UX_t + WS_{t-1})
  \]
  其中 \( f \) 可以是非线性转换函数（如 \( \tanh \)）
- \( O_t \)：时间 \( t \) 处的输出（如sigmoid/softmax输出的属于每个候选词的概率）
- \( h_t \)：时间 \( t \) 的隐藏状态
- \( U \)：输入层到隐藏层间的权重
- \( W \)：隐藏层到隐藏层的权重，负责调度记忆
- \( V \)：隐藏层到输出层，作为最后一次抽象

### RNN正向传播过程

![RNN Forward Propagation](https://github.com/user-attachments/assets/87f8a953-16ec-4962-9c89-c5f703ea09e4)

公式：
\[
S_a = UX_a + WS_{a-1}
\]
\[
h_a = f(UX_a + WS_{a-1})
\]
\[
O_a = g(V h_a)
\]

### RNN反向传播
使用链式法则，求 \( V \) 梯度（不存在和之前的状态依赖），求导->求和

- **梯度消失**（权重更新慢，训练不收敛，模型能力差，损失函数值不变等）
   - **长序列**：处理长序列时，权重绝对值较小，每一步的梯度更新较小，导致梯度随着时间步的增加而迅速减少
   - **激活函数**：RNN常用的激活函数如 \( \tanh \)，sigmoid，其导数在输入值较大或较小时接近0，这些导数值会导致梯度迅速减小
   - **初始化权重不当**
   
- **梯度爆炸**（权重更新过大，训练不收敛，模型能力差，损失函数值波动大等）
   - **长序列**：与梯度消失相反，权重绝对值较大
   - **激活函数**：输入值接近0
   - **解决办法**：设置阈值

### RNN双向传播
- **BPTT**
   - 定义：训练递归神经网络的算法
   - 实现方式：从输出端开始，沿着时间步反向计算梯度，更新网络权重

- **LSTM**
   - 定义：特殊类型RNN（解决梯度消失和梯度爆炸）
   - 实现方式：复杂化记忆细胞，控制细胞状态（控制传递信息的增加和删除）

   ![LSTM Image 1](https://github.com/user-attachments/assets/3df7ffae-4214-4c1d-bd44-2e28445f1168)
   ![LSTM Image 2](https://github.com/user-attachments/assets/9f116411-02ca-43e6-8e30-ead9b6736ccb)

- **GRU**
